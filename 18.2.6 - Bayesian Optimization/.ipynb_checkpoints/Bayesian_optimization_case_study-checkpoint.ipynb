{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.25 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from bayesian-optimization) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from bayesian-optimization) (1.7.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from bayesian-optimization) (1.15.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from catboost) (6.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\tanks\\miniconda3\\lib\\site-packages (from plotly->catboost) (1.44.0)\n",
      "Downloading bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n",
      "Using cached lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.8/102.4 MB 9.2 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 3.9/102.4 MB 9.8 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 6.0/102.4 MB 10.3 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 8.7/102.4 MB 10.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 11.0/102.4 MB 10.7 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 13.9/102.4 MB 11.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 16.8/102.4 MB 11.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 19.9/102.4 MB 12.3 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 22.3/102.4 MB 12.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.0/102.4 MB 12.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.4/102.4 MB 13.0 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 32.8/102.4 MB 13.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.4/102.4 MB 13.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.3/102.4 MB 13.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 42.7/102.4 MB 13.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 46.4/102.4 MB 14.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 49.8/102.4 MB 14.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 53.5/102.4 MB 14.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 56.6/102.4 MB 14.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 59.5/102.4 MB 14.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 62.4/102.4 MB 14.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 65.5/102.4 MB 14.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 68.7/102.4 MB 14.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 72.1/102.4 MB 14.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 75.2/102.4 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 78.9/102.4 MB 14.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.1/102.4 MB 14.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.9/102.4 MB 14.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 86.5/102.4 MB 14.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.7/102.4 MB 14.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 91.5/102.4 MB 14.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 93.1/102.4 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.9/102.4 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 96.7/102.4 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.6/102.4 MB 13.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.9/102.4 MB 13.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 102.4/102.4 MB 12.8 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm, catboost, bayesian-optimization\n",
      "Successfully installed bayesian-optimization-3.1.0 catboost-1.2.8 lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install bayesian-optimization lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Using cached bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: numpy>=2.1.3 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bayesian-optimization) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bayesian-optimization) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.14.1 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bayesian-optimization) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tanks\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.5.0)\n",
      "Using cached bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\tanks\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'Bayesian_optimization_case_study.ipynb', 'data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    simple_func,\n",
    "    {'a': (1, 3),\n",
    "    'b': (4, 7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m6.5196543\u001b[39m | \u001b[39m1.5775203\u001b[39m | \u001b[39m4.9421339\u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m8.1832102\u001b[39m | \u001b[35m2.3262402\u001b[39m | \u001b[35m5.8569700\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m7.2008305\u001b[39m | \u001b[39m1.2280504\u001b[39m | \u001b[39m5.9727801\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m5.9478749\u001b[39m | \u001b[39m1.6644144\u001b[39m | \u001b[39m4.2834605\u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m9.5266546\u001b[39m | \u001b[35m2.8865105\u001b[39m | \u001b[35m6.6401440\u001b[39m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': np.float64(2.886510525103981), 'b': np.float64(6.640144076020888)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(9.526654601124868)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two seperate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/flight_delays_train.csv')\n",
    "test_df = pd.read_csv('data/flight_delays_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "def label_enc(df_column):\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "def feature_eng(df):\n",
    "    df['flight'] = df['Origin']+df['Dest']\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "        \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       stratified=True,\n",
    "                       nfold=3)\n",
    "    return cv_result['valid auc-mean'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 5 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | num_le... | max_depth | lambda_l2 | lambda_l1 | min_ch... | min_da... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7211608\u001b[39m | \u001b[39m2195.8095\u001b[39m | \u001b[39m27.375712\u001b[39m | \u001b[39m0.0053781\u001b[39m | \u001b[39m0.0054083\u001b[39m | \u001b[39m635.16006\u001b[39m | \u001b[39m803.63664\u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.7434085\u001b[39m | \u001b[35m1190.1311\u001b[39m | \u001b[35m42.421020\u001b[39m | \u001b[35m0.0394564\u001b[39m | \u001b[35m0.0214772\u001b[39m | \u001b[35m3774.6710\u001b[39m | \u001b[35m1420.8023\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.7433697\u001b[39m | \u001b[39m2736.9213\u001b[39m | \u001b[39m33.106108\u001b[39m | \u001b[39m0.0206542\u001b[39m | \u001b[39m0.0066471\u001b[39m | \u001b[39m2112.5763\u001b[39m | \u001b[39m1444.3854\u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.7427522\u001b[39m | \u001b[39m2858.4694\u001b[39m | \u001b[39m50.427204\u001b[39m | \u001b[39m0.0003707\u001b[39m | \u001b[39m0.0390902\u001b[39m | \u001b[39m3971.4210\u001b[39m | \u001b[39m1361.3435\u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7030323\u001b[39m | \u001b[39m2393.9072\u001b[39m | \u001b[39m22.850385\u001b[39m | \u001b[39m0.0101163\u001b[39m | \u001b[39m0.0294107\u001b[39m | \u001b[39m2147.2287\u001b[39m | \u001b[39m242.78484\u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.7433321\u001b[39m | \u001b[39m1679.4687\u001b[39m | \u001b[39m49.746071\u001b[39m | \u001b[39m0.0101291\u001b[39m | \u001b[39m0.0090384\u001b[39m | \u001b[39m3531.4353\u001b[39m | \u001b[39m1818.4235\u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.7433946\u001b[39m | \u001b[39m2883.8789\u001b[39m | \u001b[39m35.936948\u001b[39m | \u001b[39m0.0290019\u001b[39m | \u001b[39m0.0168858\u001b[39m | \u001b[39m2879.0743\u001b[39m | \u001b[39m1936.7374\u001b[39m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=5, init_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': np.float64(0.7434085081497518),\n",
       " 'params': {'num_leaves': np.float64(1190.1311604629607),\n",
       "  'max_depth': np.float64(42.4210206554832),\n",
       "  'lambda_l2': np.float64(0.0394564387769446),\n",
       "  'lambda_l1': np.float64(0.021477231281508247),\n",
       "  'min_child_samples': np.float64(3774.6710613453783),\n",
       "  'min_data_in_leaf': np.float64(1420.802380782325)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': np.float64(0.721160830667969),\n",
       " 'params': {'num_leaves': np.float64(2195.8095126467),\n",
       "  'max_depth': np.float64(27.37571250509469),\n",
       "  'lambda_l2': np.float64(0.005378147788044102),\n",
       "  'lambda_l1': np.float64(0.005408398773703277),\n",
       "  'min_child_samples': np.float64(635.1600696727544),\n",
       "  'min_data_in_leaf': np.float64(803.63664285962)}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': np.float64(0.721160830667969),\n",
       "  'params': {'num_leaves': np.float64(2195.8095126467),\n",
       "   'max_depth': np.float64(27.37571250509469),\n",
       "   'lambda_l2': np.float64(0.005378147788044102),\n",
       "   'lambda_l1': np.float64(0.005408398773703277),\n",
       "   'min_child_samples': np.float64(635.1600696727544),\n",
       "   'min_data_in_leaf': np.float64(803.63664285962)}},\n",
       " {'target': np.float64(0.7434085081497518),\n",
       "  'params': {'num_leaves': np.float64(1190.1311604629607),\n",
       "   'max_depth': np.float64(42.4210206554832),\n",
       "   'lambda_l2': np.float64(0.0394564387769446),\n",
       "   'lambda_l1': np.float64(0.021477231281508247),\n",
       "   'min_child_samples': np.float64(3774.6710613453783),\n",
       "   'min_data_in_leaf': np.float64(1420.802380782325)}},\n",
       " {'target': np.float64(0.7433697591400922),\n",
       "  'params': {'num_leaves': np.float64(2736.9213417099963),\n",
       "   'max_depth': np.float64(33.10610805299051),\n",
       "   'lambda_l2': np.float64(0.020654290872170307),\n",
       "   'lambda_l1': np.float64(0.006647190609999793),\n",
       "   'min_child_samples': np.float64(2112.5763322846037),\n",
       "   'min_data_in_leaf': np.float64(1444.3854201266536)}},\n",
       " {'target': np.float64(0.7427522851213829),\n",
       "  'params': {'num_leaves': np.float64(2858.4694142006906),\n",
       "   'max_depth': np.float64(50.427204451526755),\n",
       "   'lambda_l2': np.float64(0.00037073174381276335),\n",
       "   'lambda_l1': np.float64(0.03909022641513121),\n",
       "   'min_child_samples': np.float64(3971.421010213937),\n",
       "   'min_data_in_leaf': np.float64(1361.3435904657865)}},\n",
       " {'target': np.float64(0.703032384710984),\n",
       "  'params': {'num_leaves': np.float64(2393.9072309331864),\n",
       "   'max_depth': np.float64(22.850385162212124),\n",
       "   'lambda_l2': np.float64(0.010116346274618916),\n",
       "   'lambda_l1': np.float64(0.02941078364270776),\n",
       "   'min_child_samples': np.float64(2147.2287018123156),\n",
       "   'min_data_in_leaf': np.float64(242.78484856526416)}},\n",
       " {'target': np.float64(0.7433321092967677),\n",
       "  'params': {'num_leaves': np.float64(1679.4687856374662),\n",
       "   'max_depth': np.float64(49.74607159539717),\n",
       "   'lambda_l2': np.float64(0.010129193994038332),\n",
       "   'lambda_l1': np.float64(0.009038414296535763),\n",
       "   'min_child_samples': np.float64(3531.4353045868606),\n",
       "   'min_data_in_leaf': np.float64(1818.423591436185)}},\n",
       " {'target': np.float64(0.7433946160832297),\n",
       "  'params': {'num_leaves': np.float64(2883.878956897346),\n",
       "   'max_depth': np.float64(35.93694833464771),\n",
       "   'lambda_l2': np.float64(0.02900193672857525),\n",
       "   'lambda_l1': np.float64(0.016885862392556388),\n",
       "   'min_child_samples': np.float64(2879.0743336228143),\n",
       "   'min_data_in_leaf': np.float64(1936.7374949956911)}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 1190,\n",
       " 'max_depth': 42,\n",
       " 'lambda_l2': 0.0394564387769446,\n",
       " 'lambda_l1': 0.021477231281508247,\n",
       " 'min_child_samples': 3775,\n",
       " 'min_data_in_leaf': 1421}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = lgbBO.res[1]['params']\n",
    "best_params['num_leaves'] = round(best_params['num_leaves'])\n",
    "best_params['max_depth'] = round(best_params['max_depth'])\n",
    "best_params['lambda_l2'] = float(best_params['lambda_l2'])\n",
    "best_params['lambda_l1'] = float(best_params['lambda_l1'])\n",
    "best_params['min_child_samples'] = round(best_params['min_child_samples'])\n",
    "best_params['min_data_in_leaf'] = round(best_params['min_data_in_leaf'])\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid l2-mean': [0.1514673014844385,\n",
       "  0.1492712314645639,\n",
       "  0.1474756624813826,\n",
       "  0.1459835883465858,\n",
       "  0.1447208447221237,\n",
       "  0.14368430438754562,\n",
       "  0.14273825147083763,\n",
       "  0.14203481165872095,\n",
       "  0.14131263694825724,\n",
       "  0.1407566387281432,\n",
       "  0.14027307536902556,\n",
       "  0.13981088351244875,\n",
       "  0.13940518774674399,\n",
       "  0.13905989318724118,\n",
       "  0.13873181692308204,\n",
       "  0.13843060401063412,\n",
       "  0.13821316559440003,\n",
       "  0.13798094128760213,\n",
       "  0.1377718187261209,\n",
       "  0.1375809351419043,\n",
       "  0.13742102035159107,\n",
       "  0.13730655812435766,\n",
       "  0.13716305486388758,\n",
       "  0.1370540745529002,\n",
       "  0.13693824934720533,\n",
       "  0.13683732004959162,\n",
       "  0.13675619788507867,\n",
       "  0.1366839735233893,\n",
       "  0.1365990356579297,\n",
       "  0.13652530718212208,\n",
       "  0.1364448810939738,\n",
       "  0.1363728895377551,\n",
       "  0.13630394612832666,\n",
       "  0.13623041986760578,\n",
       "  0.13615410847218748,\n",
       "  0.136086752841093,\n",
       "  0.13600615668002594,\n",
       "  0.13595436743651518,\n",
       "  0.13591292649977826,\n",
       "  0.13585111472909545,\n",
       "  0.135808125583542,\n",
       "  0.13575011512982368,\n",
       "  0.13570593956229043,\n",
       "  0.1356644148254355,\n",
       "  0.13562935804634463,\n",
       "  0.13558110252905148,\n",
       "  0.13552615999326598,\n",
       "  0.13548962346332485,\n",
       "  0.1354352581202086,\n",
       "  0.13540297406253704,\n",
       "  0.13538048322516089,\n",
       "  0.13535203013226207,\n",
       "  0.13531216822021222,\n",
       "  0.1352715596962046,\n",
       "  0.13524384135394615,\n",
       "  0.1352143482848833,\n",
       "  0.13516780143805415,\n",
       "  0.1351398282973196,\n",
       "  0.13511368669972953,\n",
       "  0.1350854316530828,\n",
       "  0.1350588587204695,\n",
       "  0.1350364753277499,\n",
       "  0.13501154838873197,\n",
       "  0.13498481682371694,\n",
       "  0.1349708720298961,\n",
       "  0.1349489145913565,\n",
       "  0.13492615345196762,\n",
       "  0.13490078583885703,\n",
       "  0.13486716427925632,\n",
       "  0.13483930439473418,\n",
       "  0.13480889610783184,\n",
       "  0.1347872669790468,\n",
       "  0.13477938075735443,\n",
       "  0.13476569074950345,\n",
       "  0.13474320793830047,\n",
       "  0.13472718510490453,\n",
       "  0.13469420615059507,\n",
       "  0.1346884339592389,\n",
       "  0.13466782262576835,\n",
       "  0.13464247738251686,\n",
       "  0.13463218497663806,\n",
       "  0.1346196970955606,\n",
       "  0.13460291159629414,\n",
       "  0.13458712753362786,\n",
       "  0.1345791531680281,\n",
       "  0.13455717634866723,\n",
       "  0.1345439333792559,\n",
       "  0.13452842284767594,\n",
       "  0.13450827291734893,\n",
       "  0.13446917715333767,\n",
       "  0.13445945817264976,\n",
       "  0.13444359781156923,\n",
       "  0.13442831443667722,\n",
       "  0.13441462635692297,\n",
       "  0.13440286369222906,\n",
       "  0.13439192161721494,\n",
       "  0.1343858891746417,\n",
       "  0.13437278627682941,\n",
       "  0.13436298464467084,\n",
       "  0.13435453818152193,\n",
       "  0.1343541904217195,\n",
       "  0.1343336864306919,\n",
       "  0.13433063439519397,\n",
       "  0.13432848022448457,\n",
       "  0.13432064703907,\n",
       "  0.13431447405144398,\n",
       "  0.13430230386058367,\n",
       "  0.13429712854505688,\n",
       "  0.13427832891130131,\n",
       "  0.13427416074612097,\n",
       "  0.13427282821267625,\n",
       "  0.134276203219188,\n",
       "  0.134265198894017,\n",
       "  0.13425008079840928,\n",
       "  0.13424023235069468,\n",
       "  0.13423568067629316,\n",
       "  0.13421971888485573,\n",
       "  0.1342168096410814,\n",
       "  0.13420177817570464,\n",
       "  0.13419732951757143,\n",
       "  0.13419143541330195,\n",
       "  0.13416563023611822,\n",
       "  0.13416665462331315,\n",
       "  0.13416052424767902,\n",
       "  0.13415418729314316,\n",
       "  0.13413600387535665,\n",
       "  0.13413672911286592,\n",
       "  0.1341298881377044,\n",
       "  0.13413308345481503,\n",
       "  0.13412827184129247,\n",
       "  0.13411951339972464,\n",
       "  0.13411376362177382,\n",
       "  0.1341042600665287,\n",
       "  0.1341101842757596,\n",
       "  0.13410395128759164,\n",
       "  0.1340957677819508,\n",
       "  0.13409554654248662,\n",
       "  0.13407858111986315,\n",
       "  0.13407542628642738,\n",
       "  0.1340770927543665,\n",
       "  0.1340693191481152,\n",
       "  0.13405947593448256,\n",
       "  0.13405400906724885,\n",
       "  0.13403886388439984,\n",
       "  0.13403051627943424,\n",
       "  0.13401604825702398,\n",
       "  0.13400337244807034,\n",
       "  0.13399474855500224,\n",
       "  0.1339875706511785,\n",
       "  0.13397340346599124,\n",
       "  0.1339549387475971,\n",
       "  0.1339544921760115,\n",
       "  0.1339435965616992,\n",
       "  0.13393808921647238,\n",
       "  0.1339316373171139,\n",
       "  0.1339291994585521,\n",
       "  0.1339252199560089,\n",
       "  0.13391030982415913,\n",
       "  0.13390283518349674,\n",
       "  0.13390314272207454,\n",
       "  0.13389939286577385,\n",
       "  0.13389987463144082,\n",
       "  0.13389419814297407,\n",
       "  0.13388213890272352,\n",
       "  0.1338801883139245,\n",
       "  0.1338651700768172,\n",
       "  0.13385708616989414,\n",
       "  0.1338473398161247,\n",
       "  0.1338404242788659,\n",
       "  0.1338379417057134,\n",
       "  0.13383458730722253,\n",
       "  0.13382530082642807,\n",
       "  0.13381336496059343,\n",
       "  0.13381488816395515,\n",
       "  0.13380857757086892,\n",
       "  0.13380174654154461,\n",
       "  0.13380667421625797,\n",
       "  0.1338007206135476,\n",
       "  0.13379527219059673,\n",
       "  0.13379646068922418,\n",
       "  0.13379372571553003,\n",
       "  0.13378448571949295,\n",
       "  0.1337896541112798,\n",
       "  0.1337878771607816,\n",
       "  0.13377456141201452,\n",
       "  0.13376324800905035,\n",
       "  0.13376148200818042,\n",
       "  0.13375459775141102,\n",
       "  0.13375114460924178,\n",
       "  0.13374911552638674,\n",
       "  0.13373729656229918,\n",
       "  0.1337283064565892,\n",
       "  0.133720798504701,\n",
       "  0.13371541237851042,\n",
       "  0.13371577351643568,\n",
       "  0.13371094003020684,\n",
       "  0.1337093964101505,\n",
       "  0.13370361084013077,\n",
       "  0.13369889002810262,\n",
       "  0.13369090018907465,\n",
       "  0.13368896844557865,\n",
       "  0.13368191544816999,\n",
       "  0.13367976351066943,\n",
       "  0.1336775019322312,\n",
       "  0.1336747393325322,\n",
       "  0.1336735814434154,\n",
       "  0.1336691776767025,\n",
       "  0.13366396898443403,\n",
       "  0.13366088442363794,\n",
       "  0.13365571423587347,\n",
       "  0.13365714223986952,\n",
       "  0.13366118372996771,\n",
       "  0.13365894861416994,\n",
       "  0.133663022744758,\n",
       "  0.1336528109641828,\n",
       "  0.13365426553262705,\n",
       "  0.133648436124538,\n",
       "  0.13364586774553458,\n",
       "  0.13364653083732905,\n",
       "  0.13364681255837857,\n",
       "  0.13364297677472484,\n",
       "  0.13363862620877237,\n",
       "  0.13363683763637843,\n",
       "  0.13363621642499451,\n",
       "  0.13362637928381282,\n",
       "  0.13362420977830247,\n",
       "  0.1336250822972104,\n",
       "  0.13361340486905887,\n",
       "  0.13361759534397127,\n",
       "  0.13360928068047614,\n",
       "  0.13361341244698755,\n",
       "  0.1336120823706647,\n",
       "  0.13360746072042765,\n",
       "  0.13360646804766985,\n",
       "  0.13361199566907686,\n",
       "  0.13362042517470035,\n",
       "  0.13362884624549481,\n",
       "  0.13362575201736288,\n",
       "  0.13362163378026568,\n",
       "  0.13361660289701494,\n",
       "  0.13360863469218748,\n",
       "  0.13361323614885007,\n",
       "  0.13360825905991003,\n",
       "  0.13361046983139463,\n",
       "  0.1336116149591741,\n",
       "  0.13361610634308024,\n",
       "  0.13361070725627527,\n",
       "  0.13360496507839983,\n",
       "  0.13360366524181616,\n",
       "  0.13360400745195572,\n",
       "  0.13359824720798733,\n",
       "  0.13359957291371016,\n",
       "  0.13359608440851428,\n",
       "  0.1336058632580463,\n",
       "  0.13360488548228777,\n",
       "  0.1336002761206708,\n",
       "  0.13360149503705757,\n",
       "  0.13359965751519726,\n",
       "  0.13360372523327144,\n",
       "  0.1336011590730452,\n",
       "  0.13360116232434577,\n",
       "  0.1336034192449508,\n",
       "  0.13359511565011029,\n",
       "  0.13359547525491453,\n",
       "  0.13360141133404085,\n",
       "  0.13360562188634698,\n",
       "  0.1336017036145075,\n",
       "  0.13360190596073152,\n",
       "  0.13360186375447866,\n",
       "  0.13361205717056063,\n",
       "  0.13360601957818327,\n",
       "  0.13359804494108393,\n",
       "  0.13359231796457646,\n",
       "  0.13359686773109267,\n",
       "  0.13359462617593482,\n",
       "  0.13359388276059728,\n",
       "  0.1335874836130185,\n",
       "  0.13358249120798002,\n",
       "  0.13358363223614192,\n",
       "  0.13357814320865252,\n",
       "  0.1335844511961418,\n",
       "  0.1335835476027427,\n",
       "  0.13358140049609854,\n",
       "  0.1335859709257703,\n",
       "  0.13358670496358221,\n",
       "  0.13358929788397447,\n",
       "  0.13358826904280627,\n",
       "  0.13358422078092516,\n",
       "  0.13359464810406116,\n",
       "  0.13359086011461693,\n",
       "  0.1335850406732751,\n",
       "  0.13358279809991908,\n",
       "  0.13358308090050425,\n",
       "  0.13358575954601662,\n",
       "  0.13358822204834428,\n",
       "  0.1335984455603295,\n",
       "  0.13360279819252902,\n",
       "  0.13361049572266712,\n",
       "  0.13361581703388753,\n",
       "  0.1336127555478341,\n",
       "  0.13361627873330156,\n",
       "  0.13361622188711747,\n",
       "  0.13361698479539802,\n",
       "  0.13362164862131967,\n",
       "  0.13362387357442995,\n",
       "  0.13362250403343898,\n",
       "  0.13361461136378464,\n",
       "  0.1336035394343577,\n",
       "  0.1335983101883043,\n",
       "  0.1335881347449695,\n",
       "  0.13359340284213295,\n",
       "  0.1335889752637694,\n",
       "  0.133586143402823,\n",
       "  0.13357802211311628,\n",
       "  0.13358302161686927,\n",
       "  0.13358987830837327,\n",
       "  0.13358945814205075,\n",
       "  0.1335873249365499,\n",
       "  0.1335897804366007,\n",
       "  0.13359177432545336,\n",
       "  0.13358963588522516,\n",
       "  0.13359533580791538,\n",
       "  0.13359746521272395,\n",
       "  0.1335965150065659,\n",
       "  0.1335962280094107,\n",
       "  0.13359514824662821,\n",
       "  0.13360105639730938,\n",
       "  0.13359674450778108,\n",
       "  0.13359800481914094,\n",
       "  0.13360101943724043,\n",
       "  0.13359869558375254,\n",
       "  0.1335937699770071,\n",
       "  0.13359674642748331,\n",
       "  0.13359282527460042,\n",
       "  0.13359300391932455,\n",
       "  0.13359088135772437,\n",
       "  0.13359318267372772,\n",
       "  0.13360085829638868,\n",
       "  0.13359974644468023,\n",
       "  0.13359635082972446,\n",
       "  0.13359669895211396,\n",
       "  0.13359989999549415,\n",
       "  0.1335951011258588,\n",
       "  0.1335946144297151,\n",
       "  0.133600103565202,\n",
       "  0.1335980978468126,\n",
       "  0.13359918218168823,\n",
       "  0.13359993349667273,\n",
       "  0.13360894108016294,\n",
       "  0.13360838654807705,\n",
       "  0.13361021849026739,\n",
       "  0.13360967663607448,\n",
       "  0.13361204634758697,\n",
       "  0.1336119408452341,\n",
       "  0.13361539480693865,\n",
       "  0.1336178497879251,\n",
       "  0.13361963219151285,\n",
       "  0.13362011897247356,\n",
       "  0.13362340972103265,\n",
       "  0.13362539809361426,\n",
       "  0.13361926939569663,\n",
       "  0.13362041333788466,\n",
       "  0.13361334474535771,\n",
       "  0.1336070710959234,\n",
       "  0.13360452227049438,\n",
       "  0.13360830329336698,\n",
       "  0.1336094607642868,\n",
       "  0.1336092540635552,\n",
       "  0.13360455966459506,\n",
       "  0.1336054723029582,\n",
       "  0.13361048603984,\n",
       "  0.13360621759584085,\n",
       "  0.13361034129777072,\n",
       "  0.13361352468509854,\n",
       "  0.13361086809571793,\n",
       "  0.13361119497464016,\n",
       "  0.133619745414771,\n",
       "  0.13362040894582522,\n",
       "  0.13362105053678336,\n",
       "  0.13361600520968162,\n",
       "  0.13361643259197387,\n",
       "  0.1336136864852518,\n",
       "  0.13362002865470898,\n",
       "  0.1336209526136813,\n",
       "  0.13361874424355633,\n",
       "  0.1336181952476381,\n",
       "  0.13361334245388792,\n",
       "  0.13361105162119385,\n",
       "  0.13361480293719705,\n",
       "  0.1336074892771414,\n",
       "  0.1336152950929753,\n",
       "  0.13361580229195907,\n",
       "  0.13361736398857166,\n",
       "  0.13362266099645775,\n",
       "  0.13361612929456082,\n",
       "  0.13362189945394073,\n",
       "  0.1336310663649579,\n",
       "  0.13363596015061618,\n",
       "  0.13364263145847105,\n",
       "  0.1336443031998924,\n",
       "  0.13364350109862527,\n",
       "  0.13364830149595489,\n",
       "  0.1336469570138014,\n",
       "  0.13364926522181034,\n",
       "  0.1336506585350255,\n",
       "  0.13365606995116897,\n",
       "  0.13365639294088472,\n",
       "  0.1336539919225118,\n",
       "  0.13365524364046705,\n",
       "  0.13365362534219977,\n",
       "  0.13365577427894135,\n",
       "  0.13365736854667815,\n",
       "  0.13365146675845288,\n",
       "  0.1336513069264697,\n",
       "  0.13365086868480866,\n",
       "  0.13365224482756624,\n",
       "  0.13365457730541375,\n",
       "  0.13364762495351176,\n",
       "  0.1336447065669148,\n",
       "  0.13364542229609971,\n",
       "  0.1336474828999999,\n",
       "  0.13365353507092415,\n",
       "  0.13366062942321377,\n",
       "  0.13366138138704597,\n",
       "  0.13365709047418725,\n",
       "  0.13365820630405068,\n",
       "  0.13366159452552254,\n",
       "  0.13366925018357248,\n",
       "  0.133667886565426,\n",
       "  0.13367001176720186,\n",
       "  0.13367072596783572,\n",
       "  0.13366727557970573,\n",
       "  0.13367028692650523,\n",
       "  0.13367547240116973,\n",
       "  0.13367088508411482,\n",
       "  0.13366146946422747,\n",
       "  0.1336677333012179,\n",
       "  0.13366938503464743,\n",
       "  0.1336706186468312,\n",
       "  0.13366318907868022,\n",
       "  0.13366905003443744,\n",
       "  0.1336681233423104,\n",
       "  0.13366791802698513,\n",
       "  0.1336705543416612,\n",
       "  0.1336734594221806,\n",
       "  0.13367323799430336,\n",
       "  0.13367537284513217,\n",
       "  0.1336796261739487,\n",
       "  0.1336797881975546,\n",
       "  0.1336782976347338,\n",
       "  0.13368052765499067,\n",
       "  0.13367792595200484,\n",
       "  0.13367891391758985,\n",
       "  0.1336761490367901,\n",
       "  0.13367814763442915,\n",
       "  0.13367919501710118,\n",
       "  0.13368229106137114,\n",
       "  0.1336879755100165,\n",
       "  0.13368627852069814,\n",
       "  0.1336939393022849,\n",
       "  0.1336964708367181,\n",
       "  0.1336916003291665,\n",
       "  0.13369596632438754,\n",
       "  0.13369927839036783,\n",
       "  0.13370024975502462,\n",
       "  0.13370073999100823,\n",
       "  0.13370721113935508,\n",
       "  0.13370890717559902,\n",
       "  0.1337169556644028,\n",
       "  0.13371368421669821,\n",
       "  0.13371653174901268,\n",
       "  0.1337240764435852,\n",
       "  0.13373148936533058,\n",
       "  0.13373473831359256,\n",
       "  0.13373754332214008,\n",
       "  0.13373413087531202,\n",
       "  0.133738274983439,\n",
       "  0.13373895349130166,\n",
       "  0.13374306203767045,\n",
       "  0.13374770007545594,\n",
       "  0.13374697143239184,\n",
       "  0.13375204207682026,\n",
       "  0.1337527936102147,\n",
       "  0.13375478358870307,\n",
       "  0.13375796644311153,\n",
       "  0.13376106132671012,\n",
       "  0.13376102320000896,\n",
       "  0.13376285938732074,\n",
       "  0.13376677051963376,\n",
       "  0.13376942858219418,\n",
       "  0.13377686182589993,\n",
       "  0.13377473646676957,\n",
       "  0.13377359187860974,\n",
       "  0.1337744968891704,\n",
       "  0.1337709783084012,\n",
       "  0.13377583098130616,\n",
       "  0.1337793066193639,\n",
       "  0.13377535145526517,\n",
       "  0.13377554866950636,\n",
       "  0.13377145171454619,\n",
       "  0.13377557113084002,\n",
       "  0.13377782926691606,\n",
       "  0.13378313526962052,\n",
       "  0.1337900610600859,\n",
       "  0.133790891096441,\n",
       "  0.13379259217776146,\n",
       "  0.1337920158943556,\n",
       "  0.13379706841028477,\n",
       "  0.1338003448325387,\n",
       "  0.13380247529372985,\n",
       "  0.13380724523714008,\n",
       "  0.1338055736002301,\n",
       "  0.1338064845559454,\n",
       "  0.13381231975978372,\n",
       "  0.13381186309613854,\n",
       "  0.1338129221793355,\n",
       "  0.1338123121540955,\n",
       "  0.1338174348151353,\n",
       "  0.13381874011390593,\n",
       "  0.1338184318551376,\n",
       "  0.13382529766312706,\n",
       "  0.13383247771161014,\n",
       "  0.13383598680739306,\n",
       "  0.13383386099521166,\n",
       "  0.13383855485714696,\n",
       "  0.1338428966959772,\n",
       "  0.13383943688223515,\n",
       "  0.13383488402730498,\n",
       "  0.13383566142533596,\n",
       "  0.1338378089022374,\n",
       "  0.13383861515258105,\n",
       "  0.13384264897288786,\n",
       "  0.13384043037279217,\n",
       "  0.13384546847728412,\n",
       "  0.13385496560679078,\n",
       "  0.1338622593759854,\n",
       "  0.1338722911564028,\n",
       "  0.1338744191880603,\n",
       "  0.13387532417698578,\n",
       "  0.13387756724852204,\n",
       "  0.13387964765721533,\n",
       "  0.133882713851405,\n",
       "  0.13388408333428686,\n",
       "  0.13389098645485048,\n",
       "  0.13389568514248001,\n",
       "  0.1338994525538693,\n",
       "  0.13390054369955923,\n",
       "  0.13389401500172932,\n",
       "  0.13389577807165162,\n",
       "  0.1338960263313359,\n",
       "  0.1338963156858946,\n",
       "  0.13390498442433477,\n",
       "  0.13390599938816206,\n",
       "  0.1339026753769839,\n",
       "  0.13390502292801748,\n",
       "  0.13391041578733787,\n",
       "  0.13391347200723883,\n",
       "  0.13390879818986906,\n",
       "  0.13390510378841053,\n",
       "  0.13390983492680802,\n",
       "  0.13391350468799917,\n",
       "  0.1339166639303701,\n",
       "  0.13391519174417493,\n",
       "  0.13391660810507294,\n",
       "  0.13391978725845485,\n",
       "  0.13391905906085028,\n",
       "  0.13392146120621046,\n",
       "  0.13392178215404005,\n",
       "  0.13392669194717366,\n",
       "  0.13392731077586656,\n",
       "  0.13393260482031882,\n",
       "  0.13393893081499744,\n",
       "  0.13394128846759398,\n",
       "  0.13393776167483712,\n",
       "  0.13393619576921473,\n",
       "  0.13393661808412644,\n",
       "  0.13394068250538602,\n",
       "  0.13393962080282834,\n",
       "  0.13394022158516594,\n",
       "  0.13394000858130917,\n",
       "  0.13393725722256566,\n",
       "  0.13393678775601653,\n",
       "  0.13394444988156248,\n",
       "  0.13394290519788726,\n",
       "  0.13394214016730532,\n",
       "  0.13394582536102612,\n",
       "  0.13394850105311273,\n",
       "  0.13395058629983964,\n",
       "  0.13395337602129279,\n",
       "  0.13395060111620535,\n",
       "  0.1339540397617485,\n",
       "  0.13395443157522688,\n",
       "  0.1339576764370209,\n",
       "  0.13395733721182104,\n",
       "  0.13396088341679344,\n",
       "  0.133967213976742,\n",
       "  0.13396327771482822,\n",
       "  0.13396318833855075,\n",
       "  0.133959059690779,\n",
       "  0.13395855573109072,\n",
       "  0.13396292101063748,\n",
       "  0.13396441477041943,\n",
       "  0.13396987958633214,\n",
       "  0.13397039344264633,\n",
       "  0.1339730971632377,\n",
       "  0.13397823408651197,\n",
       "  0.1339791689780206,\n",
       "  0.13398640760914132,\n",
       "  0.13398537054942697,\n",
       "  0.13398930621337438,\n",
       "  0.13398765198188248,\n",
       "  0.1339940187497724,\n",
       "  0.13399586885882608,\n",
       "  0.13399615957632294,\n",
       "  0.1340000985235915,\n",
       "  0.1340035730757189,\n",
       "  0.1340025687456115,\n",
       "  0.13400481146878704,\n",
       "  0.13400937552296482,\n",
       "  0.13400463694462933,\n",
       "  0.13401097184147365,\n",
       "  0.134011562316624,\n",
       "  0.13400839910987628,\n",
       "  0.13400834666029682,\n",
       "  0.13400561266138092,\n",
       "  0.13400386086580707,\n",
       "  0.13400147022182632,\n",
       "  0.13400349462347783,\n",
       "  0.13400839574493786,\n",
       "  0.13401514639706497,\n",
       "  0.13402167342678256,\n",
       "  0.13402579492765912,\n",
       "  0.13402519448677444,\n",
       "  0.13402688556364745,\n",
       "  0.13402710232355497,\n",
       "  0.13402300104083648,\n",
       "  0.13401762617967058,\n",
       "  0.13402244529637278,\n",
       "  0.1340262766065535,\n",
       "  0.13402612283166812,\n",
       "  0.13403220973590171,\n",
       "  0.1340268730451258,\n",
       "  0.13401919193367143,\n",
       "  0.13402112072877082,\n",
       "  0.1340196593526789,\n",
       "  0.1340197938596089,\n",
       "  0.13402494355049463,\n",
       "  0.13402825746302344,\n",
       "  0.13402786059635957,\n",
       "  0.1340230364604081,\n",
       "  0.13402116594002073,\n",
       "  0.13402431709490456,\n",
       "  0.13402145430489062,\n",
       "  0.13402460470822924,\n",
       "  0.1340228327233925,\n",
       "  0.1340232453013199,\n",
       "  0.13402807791556173,\n",
       "  0.1340288352863705,\n",
       "  0.13402706709431517,\n",
       "  0.1340249263328583,\n",
       "  0.13403005149838618,\n",
       "  0.13403441222394452,\n",
       "  0.13404072923342913,\n",
       "  0.13404578187970442,\n",
       "  0.13404835954283864,\n",
       "  0.134047919084515,\n",
       "  0.1340458295117539,\n",
       "  0.13404357276699336,\n",
       "  0.1340448493794453,\n",
       "  0.13404441271609518,\n",
       "  0.13405293980047964,\n",
       "  0.13405712678863066,\n",
       "  0.13405681676928355,\n",
       "  0.13405736910690466,\n",
       "  0.1340620332688919,\n",
       "  0.13406456075156606,\n",
       "  0.134070039926936,\n",
       "  0.1340705936603831,\n",
       "  0.13407348706767497,\n",
       "  0.13407535154473074,\n",
       "  0.1340735299311941,\n",
       "  0.13407532328914298,\n",
       "  0.1340767256083193,\n",
       "  0.13407588838765921,\n",
       "  0.13408108814710512,\n",
       "  0.13408136816907573,\n",
       "  0.1340787019643072,\n",
       "  0.13408330204198457,\n",
       "  0.13408390685867483,\n",
       "  0.13408677527430307,\n",
       "  0.13409072051421358,\n",
       "  0.13409495217064116,\n",
       "  0.1340936453190829,\n",
       "  0.13410233819697476,\n",
       "  0.1341066446174613,\n",
       "  0.13410595562315092,\n",
       "  0.1341092144440311,\n",
       "  0.13410564554789164,\n",
       "  0.13410770809224576,\n",
       "  0.13410852788366406,\n",
       "  0.13410564839694922,\n",
       "  0.1341042974179861,\n",
       "  0.13410471464541193,\n",
       "  0.13410547423584301,\n",
       "  0.13410799074782248,\n",
       "  0.1341140595961982,\n",
       "  0.13411509645404535,\n",
       "  0.13411899123520202,\n",
       "  0.13411676595056415,\n",
       "  0.13411914974407396,\n",
       "  0.13412298385787097,\n",
       "  0.13412682717481741,\n",
       "  0.134130765099795,\n",
       "  0.1341285540669081,\n",
       "  0.13413232309430922,\n",
       "  0.1341349755302606,\n",
       "  0.13413927053957186,\n",
       "  0.13414493771593372,\n",
       "  0.1341458315249361,\n",
       "  0.1341460774013295,\n",
       "  0.13414813693245928,\n",
       "  0.13414947887238898,\n",
       "  0.13414976861873956,\n",
       "  0.13415454886713235,\n",
       "  0.1341603890113701,\n",
       "  0.13416295003867426,\n",
       "  0.13416332735090994,\n",
       "  0.1341692703479843,\n",
       "  0.13417717546570224,\n",
       "  0.1341818308300085,\n",
       "  0.13418520480543947,\n",
       "  0.13418765942629576,\n",
       "  0.13418913315860176,\n",
       "  0.13419592977215694,\n",
       "  0.13419727701127762,\n",
       "  0.13419693025439047,\n",
       "  0.13420127906841298,\n",
       "  0.13420474500901683,\n",
       "  0.13420831084649226,\n",
       "  0.13420880706921082,\n",
       "  0.13420496679714644,\n",
       "  0.1342072154212511,\n",
       "  0.13421442515057247,\n",
       "  0.13421612716182763,\n",
       "  0.13421999715425073,\n",
       "  0.13421966846737407,\n",
       "  0.1342186207496875,\n",
       "  0.1342119794176381,\n",
       "  0.13421389215540308,\n",
       "  0.13421459250415027,\n",
       "  0.13421695877424514,\n",
       "  0.1342205213418226,\n",
       "  0.13422274053466754,\n",
       "  0.13422586156139696,\n",
       "  0.13422635154569731,\n",
       "  0.13422633766358744,\n",
       "  0.13422231144384617,\n",
       "  0.13423076900679567,\n",
       "  0.13423672014444782,\n",
       "  0.13423856350070512,\n",
       "  0.13423992210732613,\n",
       "  0.13424395267845896,\n",
       "  0.13424840041102343,\n",
       "  0.13425387291272756,\n",
       "  0.1342528696736442,\n",
       "  0.1342544101239989,\n",
       "  0.13425470818136237,\n",
       "  0.13426082163183145,\n",
       "  0.13426457461665137,\n",
       "  0.1342698255946201,\n",
       "  0.13427359397430644,\n",
       "  0.13427709224467813,\n",
       "  0.13428057358597228,\n",
       "  0.13428693863030888,\n",
       "  0.13428646885639642,\n",
       "  0.13428847296908447,\n",
       "  0.13429476644375316,\n",
       "  0.13429652480699866,\n",
       "  0.1342941736653132,\n",
       "  0.13429490434905034,\n",
       "  0.134292581688806,\n",
       "  0.13429515684840881,\n",
       "  0.13429764916366654,\n",
       "  0.1343022618023933,\n",
       "  0.13430584457689804,\n",
       "  0.13430625564358087,\n",
       "  0.1343041859602932,\n",
       "  0.13430816851149957,\n",
       "  0.1343137853332112,\n",
       "  0.1343183695325993,\n",
       "  0.134315973999115,\n",
       "  0.13431998019071698,\n",
       "  0.13432261603840684,\n",
       "  0.13431875308655272,\n",
       "  0.13432263486584778,\n",
       "  0.13432208701803594,\n",
       "  0.134327497545158,\n",
       "  0.13433347678936675,\n",
       "  0.13433313811022268,\n",
       "  0.13433377647415243,\n",
       "  0.1343342316686769,\n",
       "  0.13433715530222154,\n",
       "  0.1343372910906969,\n",
       "  0.13433927704872667,\n",
       "  0.134339068750157,\n",
       "  0.1343352233983085,\n",
       "  0.13434299795620522,\n",
       "  0.13434335696034408,\n",
       "  0.13434224865312192,\n",
       "  0.1343439534282847,\n",
       "  0.1343434942541545,\n",
       "  0.1343487851616894,\n",
       "  0.1343451368836805,\n",
       "  0.13434663825761092,\n",
       "  0.1343496673529036,\n",
       "  0.1343515747533003,\n",
       "  0.1343524852263136,\n",
       "  0.13435700959448638,\n",
       "  0.1343617986318693,\n",
       "  0.1343657269453339,\n",
       "  0.13436558544537466,\n",
       "  0.13436970814445126,\n",
       "  0.134371861130824,\n",
       "  0.1343764862013269,\n",
       "  0.13437579573437475,\n",
       "  0.1343758551277034,\n",
       "  0.13438057214085616,\n",
       "  0.13438574171728912,\n",
       "  0.13438629754076664,\n",
       "  0.13438930282649958,\n",
       "  0.1343978144704158,\n",
       "  0.13440092296617273,\n",
       "  0.13440124581007584,\n",
       "  0.13440304791201935,\n",
       "  0.13440636914574006,\n",
       "  0.13440744421887724,\n",
       "  0.1344050528284133,\n",
       "  0.13440659475722178,\n",
       "  0.13440956010203747,\n",
       "  0.1344134691712166,\n",
       "  0.13441240436367097,\n",
       "  0.1344167667433786,\n",
       "  0.13441497083130655,\n",
       "  0.13441628484571155,\n",
       "  0.13441900731745623,\n",
       "  0.13442128214387017,\n",
       "  0.13442673908786762,\n",
       "  0.13442870572286575,\n",
       "  0.13443018242530705,\n",
       "  0.13442907920217,\n",
       "  0.13443394587238627,\n",
       "  0.13443710563260552,\n",
       "  0.13443776543709396,\n",
       "  0.1344407557986361,\n",
       "  0.13444019955323103,\n",
       "  0.13444017561134952,\n",
       "  0.13444705048896816,\n",
       "  0.134449829645238,\n",
       "  0.1344526045086762,\n",
       "  0.1344532971903026,\n",
       "  0.13445145408431308,\n",
       "  0.13445302848826,\n",
       "  0.13446029077525662,\n",
       "  0.13446298934942097,\n",
       "  0.13446527562332705,\n",
       "  0.13447073467842247,\n",
       "  0.1344723699598081,\n",
       "  0.13447539195753527,\n",
       "  0.13447550800150565,\n",
       "  0.13447786998924077,\n",
       "  0.1344809907149927,\n",
       "  0.13448433203480356,\n",
       "  0.1344866319453877,\n",
       "  0.13448472951508572,\n",
       "  0.1344866516110934,\n",
       "  0.134486925577283,\n",
       "  0.13449235057955072,\n",
       "  0.13449173298852424,\n",
       "  0.13449545370786592,\n",
       "  0.13449605028913933,\n",
       "  0.1344949216451091,\n",
       "  0.13449376862749923,\n",
       "  0.13449256604199009,\n",
       "  0.1344936321424557,\n",
       "  0.13449519070365118,\n",
       "  0.1344937703091065,\n",
       "  0.13449631800636053,\n",
       "  0.1344986940365643,\n",
       "  0.13450206155041125,\n",
       "  0.13450677090415242,\n",
       "  0.1345088625308119,\n",
       "  0.13451170318204136,\n",
       "  0.13451380179873804,\n",
       "  0.13451799458652866,\n",
       "  0.1345242743157028,\n",
       "  0.13452624735002244,\n",
       "  0.13453650844070186,\n",
       "  0.13453586644497645,\n",
       "  0.13453919661835298,\n",
       "  0.1345374770415598,\n",
       "  0.134535927468853,\n",
       "  0.13453897895974973,\n",
       "  0.13454386265796206,\n",
       "  0.13454409687615793,\n",
       "  0.13454883804974238,\n",
       "  0.13454885324137272,\n",
       "  0.13454898150591998,\n",
       "  0.13454941408442314,\n",
       "  0.1345471380000129,\n",
       "  0.13454949611610054,\n",
       "  0.1345492133874612,\n",
       "  0.13455033837889138,\n",
       "  0.13455058553289145,\n",
       "  0.13455493046747377,\n",
       "  0.13455845815555997,\n",
       "  0.1345563754687227,\n",
       "  0.1345593556880072,\n",
       "  0.13456398907973535,\n",
       "  0.1345670648122442,\n",
       "  0.13457190804914881,\n",
       "  0.13457051238732692,\n",
       "  0.1345705871316069,\n",
       "  0.13457290711163816,\n",
       "  0.1345741995808531,\n",
       "  0.1345745338377717,\n",
       "  0.13457839000496366,\n",
       "  0.13458358845822385,\n",
       "  0.13458340279874453,\n",
       "  0.13458265903721953,\n",
       "  0.134586053614675,\n",
       "  0.1345966517912629,\n",
       "  0.13459797485736283,\n",
       "  0.13459846686987392,\n",
       "  0.13460077941533333,\n",
       "  0.13460169672136355,\n",
       "  0.13460626964816463,\n",
       "  0.13460755870639682,\n",
       "  0.13460725447714045,\n",
       "  0.1346102126142325,\n",
       "  0.13461144418973933,\n",
       "  0.1346154938418588,\n",
       "  0.13461911332788268,\n",
       "  0.13462381332759482,\n",
       "  0.1346273230301108,\n",
       "  0.13463097078308417,\n",
       "  0.13462866576879856,\n",
       "  0.13463260889403741,\n",
       "  0.13463767699875676,\n",
       "  0.13464440691242904,\n",
       "  0.13464493558823937,\n",
       "  0.13464301469031187,\n",
       "  0.13464584897071266,\n",
       "  0.13465242461323002,\n",
       "  0.1346581784084067,\n",
       "  0.13465938104214842,\n",
       "  0.134658813019377,\n",
       "  0.13465419494144742,\n",
       "  0.13465751044508645,\n",
       "  0.13466404671362692,\n",
       "  0.13467061571667216,\n",
       "  0.1346731437176622,\n",
       "  0.13467605072799121,\n",
       "  0.1346740349621393,\n",
       "  0.13467973669690464,\n",
       "  0.1346812888139856,\n",
       "  0.1346888351498031,\n",
       "  0.13468626989552404,\n",
       "  0.13468767018915084,\n",
       "  0.134692631159552,\n",
       "  0.134700638698091,\n",
       "  0.13469910706061658,\n",
       "  0.1347036111905338,\n",
       "  0.13470546558569385,\n",
       "  0.1347033567908365,\n",
       "  0.1347079334991309,\n",
       "  0.1347066496298458,\n",
       "  0.1347111903521901,\n",
       "  0.13471345739336568,\n",
       "  0.13471696567824795,\n",
       "  0.13471605979795162,\n",
       "  0.1347217753940849,\n",
       "  0.1347285563586616,\n",
       "  0.13473058519013614,\n",
       "  0.1347308643336756,\n",
       "  0.13473752550726115,\n",
       "  0.1347395352572785,\n",
       "  0.134742938310153,\n",
       "  0.1347476911966068,\n",
       "  0.13474778671496596,\n",
       "  0.13474812730683897,\n",
       "  0.1347482182230475,\n",
       "  0.1347498393202053,\n",
       "  0.13475441178977285,\n",
       "  0.13475846185527823,\n",
       "  0.134761818351149,\n",
       "  0.13475898605724235,\n",
       "  0.13476250254644925,\n",
       "  0.13476505155493773,\n",
       "  0.1347702263076795,\n",
       "  0.13477289561164035],\n",
       " 'valid l2-stdv': [0.0001302891205005859,\n",
       "  0.0002454826617849061,\n",
       "  0.0003726745127101155,\n",
       "  0.00041399487840211894,\n",
       "  0.00045386760425701616,\n",
       "  0.0005027483421240102,\n",
       "  0.0005724436008705232,\n",
       "  0.000594364316365359,\n",
       "  0.0006401533959754626,\n",
       "  0.0006820683846213006,\n",
       "  0.0007340820336223568,\n",
       "  0.000750402876459183,\n",
       "  0.0008023129610632187,\n",
       "  0.0008637402288743308,\n",
       "  0.0008582859338904323,\n",
       "  0.0009201225482568084,\n",
       "  0.0009499467915544409,\n",
       "  0.0009914173691097513,\n",
       "  0.001034874264066145,\n",
       "  0.0010749975057784142,\n",
       "  0.001100972903259472,\n",
       "  0.0011027827787432215,\n",
       "  0.001111304044493817,\n",
       "  0.001139553439029195,\n",
       "  0.001162720041370938,\n",
       "  0.0011704280697641602,\n",
       "  0.0011553270376124533,\n",
       "  0.0011579129673655773,\n",
       "  0.0011446123452875248,\n",
       "  0.0011326691142694455,\n",
       "  0.0011594607999824069,\n",
       "  0.001167545115640128,\n",
       "  0.0011784387174496671,\n",
       "  0.0012129866039701034,\n",
       "  0.0012162586466249704,\n",
       "  0.0012170952427953878,\n",
       "  0.001218346431070659,\n",
       "  0.0012333409058623021,\n",
       "  0.0012464878009522567,\n",
       "  0.0012586324468941913,\n",
       "  0.0012572331443536343,\n",
       "  0.0012745272638878876,\n",
       "  0.0012698797104110568,\n",
       "  0.0012651926960928867,\n",
       "  0.0012702997374226125,\n",
       "  0.001274290234718242,\n",
       "  0.0012567539910650026,\n",
       "  0.0012545242862001904,\n",
       "  0.0012788398419979316,\n",
       "  0.0012850332195674572,\n",
       "  0.0012742839282354104,\n",
       "  0.0012814151500158508,\n",
       "  0.0012848259292213466,\n",
       "  0.0012915847746617077,\n",
       "  0.0012825083782005567,\n",
       "  0.0012745669327242515,\n",
       "  0.0012941092519850924,\n",
       "  0.0012968016616982199,\n",
       "  0.001301844270664217,\n",
       "  0.0012969270760273004,\n",
       "  0.0012828374200851613,\n",
       "  0.0012737685782463007,\n",
       "  0.0012773026860345925,\n",
       "  0.0012699724089734563,\n",
       "  0.0012631754044112097,\n",
       "  0.0012614507685703167,\n",
       "  0.0012504421249587407,\n",
       "  0.0012631381816915622,\n",
       "  0.0012596228961299996,\n",
       "  0.0012691871808721568,\n",
       "  0.0012626900280281199,\n",
       "  0.0012514592317351323,\n",
       "  0.001254973200212413,\n",
       "  0.0012475361209625723,\n",
       "  0.0012394244746740549,\n",
       "  0.0012460993887533035,\n",
       "  0.0012416575598155103,\n",
       "  0.001235819694246426,\n",
       "  0.00123897327004569,\n",
       "  0.0012465805479593862,\n",
       "  0.0012326707280560381,\n",
       "  0.0012308112928680725,\n",
       "  0.0012311438117468209,\n",
       "  0.001230927404967255,\n",
       "  0.0012369188175962106,\n",
       "  0.0012413258692694886,\n",
       "  0.0012414317579980156,\n",
       "  0.0012456656879297515,\n",
       "  0.0012410867510772058,\n",
       "  0.0012464362887513356,\n",
       "  0.0012428320275005291,\n",
       "  0.0012432009533298437,\n",
       "  0.0012368969683776816,\n",
       "  0.0012308479322844174,\n",
       "  0.0012216991977394595,\n",
       "  0.0012247546733777674,\n",
       "  0.001230533513467959,\n",
       "  0.0012301288587795445,\n",
       "  0.0012337073247876728,\n",
       "  0.0012319851007005834,\n",
       "  0.001239382573800867,\n",
       "  0.0012294046027998065,\n",
       "  0.001221586669874444,\n",
       "  0.0012251667439973019,\n",
       "  0.001214356558771423,\n",
       "  0.0012156046112380213,\n",
       "  0.001219576290048681,\n",
       "  0.0012047356477245543,\n",
       "  0.0012032324047792353,\n",
       "  0.0012066942836797372,\n",
       "  0.0012059608669996026,\n",
       "  0.0012008473798414273,\n",
       "  0.001203330730835495,\n",
       "  0.0012011843200818275,\n",
       "  0.0012036372329500382,\n",
       "  0.0012059548159377177,\n",
       "  0.0012199163937404163,\n",
       "  0.0012160610324367703,\n",
       "  0.001216509065104685,\n",
       "  0.0012214790234439732,\n",
       "  0.001224304853608616,\n",
       "  0.0012278362095628213,\n",
       "  0.0012238683113445724,\n",
       "  0.0012218847303190584,\n",
       "  0.0012205928653567747,\n",
       "  0.0012333893835562108,\n",
       "  0.001234802643187014,\n",
       "  0.0012294706319192254,\n",
       "  0.0012272526714584729,\n",
       "  0.001223301156998333,\n",
       "  0.001216008429702555,\n",
       "  0.001209784430246888,\n",
       "  0.0012094639210193036,\n",
       "  0.0012173648192173713,\n",
       "  0.0012103609667889787,\n",
       "  0.001216842932706419,\n",
       "  0.001212963438486517,\n",
       "  0.0012186119142947893,\n",
       "  0.00121835691655804,\n",
       "  0.0012160087693940036,\n",
       "  0.00121656093587846,\n",
       "  0.0012129344864042942,\n",
       "  0.001210808796812803,\n",
       "  0.0012192805950025382,\n",
       "  0.0012153228298660433,\n",
       "  0.0012164254019854,\n",
       "  0.0012179179630277565,\n",
       "  0.0012231797502220958,\n",
       "  0.0012304689216652517,\n",
       "  0.0012307925325751423,\n",
       "  0.0012317115016934174,\n",
       "  0.0012194450114069158,\n",
       "  0.0012179835880703344,\n",
       "  0.0012124167682288425,\n",
       "  0.001217922793139385,\n",
       "  0.0012126697853656427,\n",
       "  0.0012141885490885007,\n",
       "  0.001219733641459297,\n",
       "  0.0012161544644916909,\n",
       "  0.001208699359014881,\n",
       "  0.0012096655321814712,\n",
       "  0.0012045927506966346,\n",
       "  0.0012043439174765746,\n",
       "  0.001193284313825937,\n",
       "  0.0011880556239199608,\n",
       "  0.0011877150572715146,\n",
       "  0.001194231025621691,\n",
       "  0.0011869947703199708,\n",
       "  0.001181152432630315,\n",
       "  0.0011814650570322108,\n",
       "  0.0011782210088364586,\n",
       "  0.001170680574451374,\n",
       "  0.0011687522562258445,\n",
       "  0.001175240111074902,\n",
       "  0.0011882817576909783,\n",
       "  0.0011886457639245826,\n",
       "  0.0011831650461930183,\n",
       "  0.0011852240969370657,\n",
       "  0.001185959512098288,\n",
       "  0.0011856590946763414,\n",
       "  0.0011885743306083284,\n",
       "  0.001193531114475833,\n",
       "  0.001197475398896713,\n",
       "  0.0011952986434604089,\n",
       "  0.0011968353461650988,\n",
       "  0.0011957650322925388,\n",
       "  0.0011933890544791811,\n",
       "  0.0011931919549246586,\n",
       "  0.0011913336832143375,\n",
       "  0.001202731274548716,\n",
       "  0.001201765370668121,\n",
       "  0.0012026215337530913,\n",
       "  0.0012012576478840054,\n",
       "  0.0011998279310535401,\n",
       "  0.0012057091267778472,\n",
       "  0.0012160327801402644,\n",
       "  0.0012121200900726634,\n",
       "  0.001199164183594678,\n",
       "  0.001196954382474445,\n",
       "  0.0011945543806221122,\n",
       "  0.0011901502941980703,\n",
       "  0.0011954557772102306,\n",
       "  0.0011998965718819996,\n",
       "  0.0012098260802204561,\n",
       "  0.0012093400354018914,\n",
       "  0.0012053829784463659,\n",
       "  0.001207406390939134,\n",
       "  0.001201122145343893,\n",
       "  0.0012021877653205805,\n",
       "  0.0012034435671892355,\n",
       "  0.001203540965005418,\n",
       "  0.0012057339973410572,\n",
       "  0.0012113472198539837,\n",
       "  0.0012194264998984512,\n",
       "  0.001223045292381144,\n",
       "  0.0012230942424282173,\n",
       "  0.0012255356300458946,\n",
       "  0.0012264435328629983,\n",
       "  0.0012293473039002153,\n",
       "  0.001230681249298202,\n",
       "  0.0012229752758186492,\n",
       "  0.0012300188151652066,\n",
       "  0.001233396072116527,\n",
       "  0.0012396641873655672,\n",
       "  0.0012421918351197532,\n",
       "  0.0012415751139297178,\n",
       "  0.0012409642727063165,\n",
       "  0.0012528493596759033,\n",
       "  0.0012479516120384392,\n",
       "  0.0012544975683516887,\n",
       "  0.001253534844488389,\n",
       "  0.0012509073602817943,\n",
       "  0.0012512968595766085,\n",
       "  0.0012499667437082364,\n",
       "  0.0012486730633884343,\n",
       "  0.0012520955432121887,\n",
       "  0.0012501099889993567,\n",
       "  0.0012575917407443276,\n",
       "  0.0012575074454145027,\n",
       "  0.0012657639571072544,\n",
       "  0.0012711158922588297,\n",
       "  0.0012668930708556261,\n",
       "  0.001259359132883648,\n",
       "  0.0012579229679599614,\n",
       "  0.0012628286087295005,\n",
       "  0.0012642774098236059,\n",
       "  0.0012628463626147353,\n",
       "  0.0012537930488800237,\n",
       "  0.0012587381223293662,\n",
       "  0.0012479033197868206,\n",
       "  0.001245350149356897,\n",
       "  0.0012533856776942413,\n",
       "  0.0012494645983986755,\n",
       "  0.0012524931595444024,\n",
       "  0.001250168114080333,\n",
       "  0.0012499897043094068,\n",
       "  0.0012542158048282816,\n",
       "  0.0012575964680432016,\n",
       "  0.001256910984089501,\n",
       "  0.0012580240742347917,\n",
       "  0.0012591735339504365,\n",
       "  0.001261754220274323,\n",
       "  0.001261658633382433,\n",
       "  0.001261658058919431,\n",
       "  0.0012562299013031235,\n",
       "  0.0012578026444024064,\n",
       "  0.0012575390112519486,\n",
       "  0.001248846852764104,\n",
       "  0.0012483439475968654,\n",
       "  0.0012491589042852359,\n",
       "  0.0012418349216972626,\n",
       "  0.0012353481289989487,\n",
       "  0.0012368923838177634,\n",
       "  0.0012366094788148199,\n",
       "  0.0012381510388835585,\n",
       "  0.0012402693421392419,\n",
       "  0.001240383170122631,\n",
       "  0.0012442803887026037,\n",
       "  0.0012401274884071625,\n",
       "  0.0012396268723892533,\n",
       "  0.0012496479932001176,\n",
       "  0.0012437800100736848,\n",
       "  0.0012493068880805116,\n",
       "  0.0012482898920712158,\n",
       "  0.001243559981739413,\n",
       "  0.0012458173465289297,\n",
       "  0.001253229138348731,\n",
       "  0.001255767744573548,\n",
       "  0.0012583238960073605,\n",
       "  0.0012528793499614616,\n",
       "  0.0012528782058083682,\n",
       "  0.00125931080824776,\n",
       "  0.0012579442810069156,\n",
       "  0.0012627125205670394,\n",
       "  0.0012679310891699062,\n",
       "  0.001275200686931502,\n",
       "  0.0012717343971544626,\n",
       "  0.0012767034235085348,\n",
       "  0.001285245566082498,\n",
       "  0.001285984648281769,\n",
       "  0.0012846258234159664,\n",
       "  0.0012799901220808749,\n",
       "  0.0012802848459071774,\n",
       "  0.0012807453370154113,\n",
       "  0.0012789664914139515,\n",
       "  0.001271880692218402,\n",
       "  0.0012779644978249873,\n",
       "  0.0012795140315913583,\n",
       "  0.0012831717921227854,\n",
       "  0.0012765595433465735,\n",
       "  0.0012703018996173458,\n",
       "  0.0012696799634026737,\n",
       "  0.0012748693458334056,\n",
       "  0.0012761275687163932,\n",
       "  0.0012746746236908183,\n",
       "  0.0012729141196471218,\n",
       "  0.0012699548908202615,\n",
       "  0.0012665884483620139,\n",
       "  0.001272266873906292,\n",
       "  0.001274815670459103,\n",
       "  0.0012739573081669165,\n",
       "  0.001270535195704956,\n",
       "  0.001272119910755596,\n",
       "  0.0012750451914248218,\n",
       "  0.0012686869206150041,\n",
       "  0.0012694121784747078,\n",
       "  0.0012654825332605057,\n",
       "  0.0012640868455421881,\n",
       "  0.001263163282383505,\n",
       "  0.0012652302491673929,\n",
       "  0.0012723648895799926,\n",
       "  0.0012773875328313743,\n",
       "  0.0012733337077189665,\n",
       "  0.0012796878534977096,\n",
       "  0.0012779833088052586,\n",
       "  0.00127111237884507,\n",
       "  0.0012680146180307761,\n",
       "  0.001268018676955029,\n",
       "  0.001269853251924826,\n",
       "  0.0012736945770772997,\n",
       "  0.001274256377939331,\n",
       "  0.0012773442761390046,\n",
       "  0.0012734293549505342,\n",
       "  0.001271758184505573,\n",
       "  0.0012704587426625399,\n",
       "  0.0012710496635015577,\n",
       "  0.0012721277155749975,\n",
       "  0.00126747582666944,\n",
       "  0.0012673659674033448,\n",
       "  0.0012671456789692752,\n",
       "  0.0012657799090302155,\n",
       "  0.0012661358473536168,\n",
       "  0.0012622610011954658,\n",
       "  0.0012617891933471843,\n",
       "  0.0012643876372491788,\n",
       "  0.0012632206436844996,\n",
       "  0.0012619961157249555,\n",
       "  0.0012662781743094144,\n",
       "  0.0012650575928744067,\n",
       "  0.001261791899692576,\n",
       "  0.0012603801519699726,\n",
       "  0.0012569241580970402,\n",
       "  0.001251577669061834,\n",
       "  0.0012504152774977123,\n",
       "  0.0012516792160593942,\n",
       "  0.0012554972833921912,\n",
       "  0.0012522256455858875,\n",
       "  0.0012484812159122608,\n",
       "  0.0012487425879522454,\n",
       "  0.0012524786749461407,\n",
       "  0.0012517009944652613,\n",
       "  0.0012477894212921355,\n",
       "  0.0012480733275416175,\n",
       "  0.0012430170851101836,\n",
       "  0.001245026913254749,\n",
       "  0.0012442212946460925,\n",
       "  0.0012433421407910307,\n",
       "  0.001243007340378525,\n",
       "  0.001243659658368381,\n",
       "  0.001233402405682635,\n",
       "  0.0012329927183584977,\n",
       "  0.0012316189807139602,\n",
       "  0.0012277976270220407,\n",
       "  0.0012278525475140321,\n",
       "  0.0012249980086843598,\n",
       "  0.0012163847431148145,\n",
       "  0.0012185747691755304,\n",
       "  0.0012177968200066918,\n",
       "  0.0012120380015964776,\n",
       "  0.0012109629244631596,\n",
       "  0.0012096382548278922,\n",
       "  0.001213223103412415,\n",
       "  0.001217292687566113,\n",
       "  0.0012187567557403716,\n",
       "  0.0012099174534079844,\n",
       "  0.0012075852506358935,\n",
       "  0.0012049180823702043,\n",
       "  0.001197164207834827,\n",
       "  0.0011982204253274354,\n",
       "  0.0012011934326464475,\n",
       "  0.0011990665078328482,\n",
       "  0.0011993856217623804,\n",
       "  0.0011945029939521064,\n",
       "  0.0011960278376832337,\n",
       "  0.0012002406882084251,\n",
       "  0.0011966403488329613,\n",
       "  0.0012010668232959897,\n",
       "  0.0012001402872156162,\n",
       "  0.001201495560904484,\n",
       "  0.0012047201405727106,\n",
       "  0.0012067174009982268,\n",
       "  0.0012110753049993626,\n",
       "  0.001211988903799619,\n",
       "  0.0012087558400598474,\n",
       "  0.0012085378055331185,\n",
       "  0.0012078051794488377,\n",
       "  0.0012083581866409703,\n",
       "  0.00120149278087548,\n",
       "  0.0011988382533898022,\n",
       "  0.0011997095111362843,\n",
       "  0.0011951825960524818,\n",
       "  0.0011954124288604225,\n",
       "  0.0011985240039291297,\n",
       "  0.0012058762979833238,\n",
       "  0.001209327946326393,\n",
       "  0.00120835854067931,\n",
       "  0.0012041851374429773,\n",
       "  0.001204014867512697,\n",
       "  0.001198320553383151,\n",
       "  0.0011955059106335438,\n",
       "  0.0011935606812436897,\n",
       "  0.0011924900731067154,\n",
       "  0.0011958266940847219,\n",
       "  0.001201813155293862,\n",
       "  0.0012023812649419798,\n",
       "  0.0012038942550438151,\n",
       "  0.0012013668306885663,\n",
       "  0.0011957509689402982,\n",
       "  0.0012074397562286368,\n",
       "  0.0012030492443845236,\n",
       "  0.0011982533557381855,\n",
       "  0.0011967435797382055,\n",
       "  0.00120178663549437,\n",
       "  0.0011982747273759293,\n",
       "  0.0012013800059235765,\n",
       "  0.0011939380593484405,\n",
       "  0.0011913386875970027,\n",
       "  0.0011905685004249867,\n",
       "  0.001192596505578432,\n",
       "  0.0011966375465839786,\n",
       "  0.001195167015177735,\n",
       "  0.0011945425069906383,\n",
       "  0.001189954080071329,\n",
       "  0.0011956300226779156,\n",
       "  0.0011938131585044703,\n",
       "  0.0011971469443009517,\n",
       "  0.0011977189382684718,\n",
       "  0.0011994739999122497,\n",
       "  0.0011988496713080715,\n",
       "  0.0012004677327643652,\n",
       "  0.0011921967789082822,\n",
       "  0.0011880762973356868,\n",
       "  0.001195975125011859,\n",
       "  0.0011957936455354184,\n",
       "  0.0012010312727689292,\n",
       "  0.001195948337606465,\n",
       "  0.0011994681665086658,\n",
       "  0.0011988851697900694,\n",
       "  0.0012021374061803453,\n",
       "  0.0012014421381882278,\n",
       "  0.0012062676015399114,\n",
       "  0.001204823884300908,\n",
       "  0.0012051496636161863,\n",
       "  0.0012042767892886564,\n",
       "  0.001204971526264308,\n",
       "  0.0012079359489129238,\n",
       "  0.0012106467986485437,\n",
       "  0.00120428349962254,\n",
       "  0.0012010886668172695,\n",
       "  0.001201790182091767,\n",
       "  0.0012012066796685285,\n",
       "  0.00119726349667913,\n",
       "  0.001193597958186568,\n",
       "  0.0011916431302216872,\n",
       "  0.0011936099427467149,\n",
       "  0.0011946945437637576,\n",
       "  0.001192183850977448,\n",
       "  0.0011935533204617574,\n",
       "  0.001194860279340506,\n",
       "  0.0011950570741045374,\n",
       "  0.0011923552811372304,\n",
       "  0.0011919725111943934,\n",
       "  0.0011928565826545042,\n",
       "  0.0011934926659889335,\n",
       "  0.0011960171770835411,\n",
       "  0.0011941859174118214,\n",
       "  0.0012004058298190807,\n",
       "  0.001200211107268379,\n",
       "  0.001195455863947905,\n",
       "  0.0011970025989090929,\n",
       "  0.0011953350672274118,\n",
       "  0.0011937212252860314,\n",
       "  0.001193370816863698,\n",
       "  0.0011941293872074301,\n",
       "  0.0011857391117695004,\n",
       "  0.001185032957359364,\n",
       "  0.001183493422960341,\n",
       "  0.0011808491402075507,\n",
       "  0.0011774796708053994,\n",
       "  0.0011706556377867623,\n",
       "  0.0011718969153105166,\n",
       "  0.0011676752249003492,\n",
       "  0.001171035683129219,\n",
       "  0.0011737994958644592,\n",
       "  0.0011736651432083466,\n",
       "  0.0011774141618737038,\n",
       "  0.0011777350168309822,\n",
       "  0.0011725850029583318,\n",
       "  0.0011677781421105837,\n",
       "  0.0011642030616689886,\n",
       "  0.001162912124452807,\n",
       "  0.0011629180914428275,\n",
       "  0.0011620335884772732,\n",
       "  0.0011579523923905158,\n",
       "  0.0011553431499228982,\n",
       "  0.0011535365200710248,\n",
       "  0.0011489320278560851,\n",
       "  0.0011517592043400178,\n",
       "  0.001152685101631696,\n",
       "  0.0011587897059148265,\n",
       "  0.0011577271997268085,\n",
       "  0.0011596917763809012,\n",
       "  0.0011606466029582538,\n",
       "  0.001163358885341225,\n",
       "  0.0011640589989490606,\n",
       "  0.0011688083499764132,\n",
       "  0.0011661411401158444,\n",
       "  0.0011710004442754992,\n",
       "  0.0011690170856732422,\n",
       "  0.0011651702048201637,\n",
       "  0.0011660641546245715,\n",
       "  0.0011658991481274196,\n",
       "  0.0011649649245449962,\n",
       "  0.001158050587534403,\n",
       "  0.001161565959386072,\n",
       "  0.0011648556498934,\n",
       "  0.0011638865458936823,\n",
       "  0.0011642155921686441,\n",
       "  0.0011654838932110665,\n",
       "  0.0011657687442612487,\n",
       "  0.0011660087593668709,\n",
       "  0.0011628981624916387,\n",
       "  0.0011622863025603529,\n",
       "  0.001166674235426288,\n",
       "  0.0011734758110276808,\n",
       "  0.001174536925896369,\n",
       "  0.0011726488796673545,\n",
       "  0.001169010763675875,\n",
       "  0.0011659054752385435,\n",
       "  0.0011651721394030279,\n",
       "  0.001165946602162966,\n",
       "  0.0011685622176021207,\n",
       "  0.0011730623634881143,\n",
       "  0.0011737931519028304,\n",
       "  0.0011725898367190872,\n",
       "  0.0011729150785751746,\n",
       "  0.001173257483466493,\n",
       "  0.0011656545701684699,\n",
       "  0.0011616697390897234,\n",
       "  0.0011602801507421643,\n",
       "  0.0011655232245331019,\n",
       "  0.0011615959987227092,\n",
       "  0.0011691635009693526,\n",
       "  0.0011667114750486215,\n",
       "  0.0011712473509835981,\n",
       "  0.0011694782258465403,\n",
       "  0.0011706121737294485,\n",
       "  0.001172345922590159,\n",
       "  0.0011692334936029176,\n",
       "  0.0011706199666379935,\n",
       "  0.0011732971231015318,\n",
       "  0.0011733201888434405,\n",
       "  0.0011677511278513377,\n",
       "  0.0011684378261167007,\n",
       "  0.0011696826418458731,\n",
       "  0.001165468983621102,\n",
       "  0.0011660575505509367,\n",
       "  0.0011649218369204394,\n",
       "  0.0011639012023633149,\n",
       "  0.0011576609073938606,\n",
       "  0.001158462725944962,\n",
       "  0.0011654137978933182,\n",
       "  0.0011646814439199413,\n",
       "  0.0011590052946881743,\n",
       "  0.001154926567400823,\n",
       "  0.0011504799537532665,\n",
       "  0.001151685811795276,\n",
       "  0.0011575137741950956,\n",
       "  0.0011595144472776066,\n",
       "  0.0011586027846674878,\n",
       "  0.0011558193066711657,\n",
       "  0.001159880971696524,\n",
       "  0.0011558479878558691,\n",
       "  0.0011524692620336846,\n",
       "  0.0011540616784887318,\n",
       "  0.0011567094982631,\n",
       "  0.0011557783481742053,\n",
       "  0.0011656457765055006,\n",
       "  0.0011678393692595246,\n",
       "  0.0011677542094349822,\n",
       "  0.00116765734167817,\n",
       "  0.0011663714134584476,\n",
       "  0.001161999933476015,\n",
       "  0.0011622159167682005,\n",
       "  0.0011634047135387527,\n",
       "  0.0011594180538342005,\n",
       "  0.001153267161832213,\n",
       "  0.0011526926858684552,\n",
       "  0.0011520101119950864,\n",
       "  0.0011556620869379985,\n",
       "  0.0011506341764110897,\n",
       "  0.0011544033738661473,\n",
       "  0.001156053203277629,\n",
       "  0.0011585485554041128,\n",
       "  0.0011598006365697235,\n",
       "  0.0011533887123142473,\n",
       "  0.001152897041596518,\n",
       "  0.0011494745703162436,\n",
       "  0.0011502873794501619,\n",
       "  0.0011517774784225335,\n",
       "  0.001150387635313217,\n",
       "  0.0011548998093877944,\n",
       "  0.0011591106590379875,\n",
       "  0.0011608627077547367,\n",
       "  0.001164078534316793,\n",
       "  0.0011662142342705537,\n",
       "  0.0011699834926155411,\n",
       "  0.0011706401241274437,\n",
       "  0.0011679999100299816,\n",
       "  0.0011632327301364812,\n",
       "  0.0011612168897187456,\n",
       "  0.0011576480161060465,\n",
       "  0.001156563618583698,\n",
       "  0.0011580819040830006,\n",
       "  0.0011588402757654864,\n",
       "  0.001166062754202924,\n",
       "  0.0011613456572319848,\n",
       "  0.0011596046779028938,\n",
       "  0.0011581906240816999,\n",
       "  0.0011559972408230277,\n",
       "  0.001154743375598828,\n",
       "  0.001155020976847429,\n",
       "  0.0011483496991686812,\n",
       "  0.0011455463367625255,\n",
       "  0.0011474493331464501,\n",
       "  0.0011496253317628076,\n",
       "  0.001149628687960137,\n",
       "  0.0011508611255929117,\n",
       "  0.0011503323792150268,\n",
       "  0.001150016946288703,\n",
       "  0.0011540473608942874,\n",
       "  0.0011540880555690712,\n",
       "  0.0011558143540662538,\n",
       "  0.0011546015141037412,\n",
       "  0.00115465294150465,\n",
       "  0.0011526129249791964,\n",
       "  0.0011538047126119545,\n",
       "  0.0011530455096056683,\n",
       "  0.0011506824283535047,\n",
       "  0.0011498199115087867,\n",
       "  0.0011549266884668852,\n",
       "  0.0011500911049816978,\n",
       "  0.0011496703484456202,\n",
       "  0.0011500762716144545,\n",
       "  0.0011506035994736923,\n",
       "  0.0011496663592011137,\n",
       "  0.0011455765100177673,\n",
       "  0.0011471789989693161,\n",
       "  0.0011463797255945333,\n",
       "  0.0011523085234968304,\n",
       "  0.0011503629720714047,\n",
       "  0.0011504901166370398,\n",
       "  0.00114972776873293,\n",
       "  0.0011471403385777124,\n",
       "  0.0011461926073083046,\n",
       "  0.001149684569056907,\n",
       "  0.0011530825016079372,\n",
       "  0.0011552003957259582,\n",
       "  0.0011567721609926547,\n",
       "  0.001155670803911125,\n",
       "  0.001156305902852469,\n",
       "  0.0011559675140209997,\n",
       "  0.0011561019498269626,\n",
       "  0.0011556639379868398,\n",
       "  0.001159656073753162,\n",
       "  0.0011586142988019904,\n",
       "  0.001160025362087498,\n",
       "  0.0011584719799892747,\n",
       "  0.0011540527119729826,\n",
       "  0.0011543355102190034,\n",
       "  0.0011563092439357552,\n",
       "  0.0011572461570825375,\n",
       "  0.0011539035901787704,\n",
       "  0.0011531504047329578,\n",
       "  0.0011556801498785695,\n",
       "  0.0011519273587786544,\n",
       "  0.0011529002775696026,\n",
       "  0.001152783395200324,\n",
       "  0.0011477789680252942,\n",
       "  0.0011472551098421213,\n",
       "  0.001145459680569682,\n",
       "  0.0011392493229510777,\n",
       "  0.0011386573023690189,\n",
       "  0.0011386696889515414,\n",
       "  0.001141503009402379,\n",
       "  0.0011379295995768818,\n",
       "  0.0011393177860654035,\n",
       "  0.0011386792338557961,\n",
       "  0.001137641318259541,\n",
       "  0.0011389225862095548,\n",
       "  0.001135190503619628,\n",
       "  0.0011355730725688383,\n",
       "  0.0011344294834996873,\n",
       "  0.001130563045342291,\n",
       "  0.0011297532707201401,\n",
       "  0.0011298824103604527,\n",
       "  0.0011312634730696399,\n",
       "  0.0011295747727695178,\n",
       "  0.0011251740714835281,\n",
       "  0.001125177373830546,\n",
       "  0.001123922400884489,\n",
       "  0.0011263749732609279,\n",
       "  0.0011246397183724195,\n",
       "  0.0011285144679058763,\n",
       "  0.0011246717902905186,\n",
       "  0.0011270231854223157,\n",
       "  0.0011290514434285645,\n",
       "  0.0011293904552964254,\n",
       "  0.0011306901488019202,\n",
       "  0.0011338771747889202,\n",
       "  0.0011357522338293596,\n",
       "  0.0011316574363342397,\n",
       "  0.0011298517448402534,\n",
       "  0.0011335458317886106,\n",
       "  0.0011388784788634299,\n",
       "  0.0011450577646901691,\n",
       "  0.0011492716777288822,\n",
       "  0.0011531078365240811,\n",
       "  0.0011560314189552113,\n",
       "  0.0011562531720352232,\n",
       "  0.0011522974209939479,\n",
       "  0.0011519300359154922,\n",
       "  0.0011478096068656793,\n",
       "  0.0011514012672846523,\n",
       "  0.0011517802790748488,\n",
       "  0.0011587187453109607,\n",
       "  0.0011570468572991302,\n",
       "  0.0011612474120420862,\n",
       "  0.0011607229180111915,\n",
       "  0.0011654138587303788,\n",
       "  0.0011615097077774127,\n",
       "  0.0011581263862145046,\n",
       "  0.0011557755371342472,\n",
       "  0.0011614035674091536,\n",
       "  0.0011625195223730622,\n",
       "  0.0011600268429618362,\n",
       "  0.0011608839037434658,\n",
       "  0.0011693680148559028,\n",
       "  0.0011699925900568306,\n",
       "  0.001167230285370812,\n",
       "  0.0011661240367037734,\n",
       "  0.0011629753119348429,\n",
       "  0.00116250852331329,\n",
       "  0.0011643663369219027,\n",
       "  0.0011697101220179737,\n",
       "  0.0011665175198096024,\n",
       "  0.0011675031555062455,\n",
       "  0.0011668708353353232,\n",
       "  0.0011633863573437145,\n",
       "  0.0011623042572295457,\n",
       "  0.001163798624476803,\n",
       "  0.0011627101056178712,\n",
       "  0.0011619569338664202,\n",
       "  0.0011628214132953284,\n",
       "  0.001162780830830439,\n",
       "  0.001165060251959731,\n",
       "  0.0011609303574699967,\n",
       "  0.001165034563245162,\n",
       "  0.0011662902970665863,\n",
       "  0.0011654419635067394,\n",
       "  0.0011677244742909411,\n",
       "  0.0011649689519441597,\n",
       "  0.0011677830836631944,\n",
       "  0.0011663233591890562,\n",
       "  0.0011679838203622404,\n",
       "  0.0011685261367767722,\n",
       "  0.00116558755076821,\n",
       "  0.0011646485420803358,\n",
       "  0.001163195413767615,\n",
       "  0.0011585761223809373,\n",
       "  0.0011558896399856563,\n",
       "  0.0011498496279114545,\n",
       "  0.0011510753423057028,\n",
       "  0.00115050714957413,\n",
       "  0.0011482345840866835,\n",
       "  0.0011462176474571683,\n",
       "  0.0011425354544207461,\n",
       "  0.0011430013000870616,\n",
       "  0.001144630224936293,\n",
       "  0.0011525059674683434,\n",
       "  0.0011520429163275102,\n",
       "  0.001145449447630373,\n",
       "  0.0011439432833059047,\n",
       "  0.0011485695241470629,\n",
       "  0.0011475154506239728,\n",
       "  0.0011436202940877155,\n",
       "  0.0011407295428586487,\n",
       "  0.0011431590524027095,\n",
       "  0.0011474301288093445,\n",
       "  0.001148404322860357,\n",
       "  0.0011482094582010393,\n",
       "  0.001145306772522551,\n",
       "  0.0011430747459307375,\n",
       "  0.0011453722334705313,\n",
       "  0.0011465772704312624,\n",
       "  0.0011479681639631127,\n",
       "  0.0011434642694693297,\n",
       "  0.0011390405833133373,\n",
       "  0.0011339248765660879,\n",
       "  0.0011311406902159734,\n",
       "  0.0011354423662397217,\n",
       "  0.0011366489055962157,\n",
       "  0.0011333883069237223,\n",
       "  0.001133962306804156,\n",
       "  0.0011388670559961398,\n",
       "  0.0011401027573951833,\n",
       "  0.0011423269065621575,\n",
       "  0.0011424624428854314,\n",
       "  0.0011428146525268666,\n",
       "  0.0011363296782069781,\n",
       "  0.001141055994179647,\n",
       "  0.0011377020760476833,\n",
       "  0.0011382614023858484,\n",
       "  0.0011410499466722453,\n",
       "  0.0011377229795175028,\n",
       "  0.0011427144767301646,\n",
       "  0.0011424341437468973,\n",
       "  0.0011414691235571816,\n",
       "  0.0011406346588307304,\n",
       "  0.0011369976518451055,\n",
       "  0.0011371392406364306,\n",
       "  0.0011385702606090571,\n",
       "  0.001140511279484799,\n",
       "  0.0011379801830637202,\n",
       "  0.0011331388171646186,\n",
       "  0.0011336026595756452,\n",
       "  0.0011332322744149303,\n",
       "  0.0011340366298982496,\n",
       "  0.00113779527930867,\n",
       "  0.0011331725926937502,\n",
       "  0.0011343635169093849,\n",
       "  0.0011324367945614758,\n",
       "  0.0011356743471546997,\n",
       "  0.0011333233351461971,\n",
       "  0.001132653151724887,\n",
       "  0.0011349622614037515,\n",
       "  0.001135754832365609,\n",
       "  0.0011374451618084006,\n",
       "  0.0011335256123797216,\n",
       "  0.0011378525120902083,\n",
       "  0.0011341227465080602,\n",
       "  0.0011379773481864732,\n",
       "  0.001136674838302147,\n",
       "  0.001133741123758698,\n",
       "  0.0011307057287475429,\n",
       "  0.0011348542892607254,\n",
       "  0.0011363287429239117,\n",
       "  0.0011374980731154657,\n",
       "  0.0011310130553958948,\n",
       "  0.0011313439018385998,\n",
       "  0.0011228218090979965,\n",
       "  0.001123228846117469,\n",
       "  0.001122155846761643,\n",
       "  0.001119941129292949,\n",
       "  0.0011175719682581106,\n",
       "  0.0011166853120717888,\n",
       "  0.0011133423545548779,\n",
       "  0.001115368336532337,\n",
       "  0.0011155517825411326,\n",
       "  0.0011168489329315676,\n",
       "  0.001111999445800862,\n",
       "  0.0011164782155162433,\n",
       "  0.0011186334792165017,\n",
       "  0.001120112100856948,\n",
       "  0.00111635799093064,\n",
       "  0.0011150753156405054,\n",
       "  0.0011125010054309614,\n",
       "  0.0011143375297540522,\n",
       "  0.0011180716603659566,\n",
       "  0.0011133645641197788,\n",
       "  0.001120825804773224,\n",
       "  0.0011260255982721834,\n",
       "  0.00112634936205038,\n",
       "  0.0011225946910483012,\n",
       "  0.001115844135682454,\n",
       "  0.0011191952361562754,\n",
       "  0.0011171502397274821,\n",
       "  0.001113031490789509,\n",
       "  0.0011183036771879116,\n",
       "  0.0011172008574580733,\n",
       "  0.0011203450429728512,\n",
       "  0.0011166752091982927,\n",
       "  0.001118585111350093,\n",
       "  0.00111421821320987,\n",
       "  0.0011070216545037137,\n",
       "  0.0011090714104925467,\n",
       "  0.0011110152027322836,\n",
       "  0.0011055257005298158,\n",
       "  0.0011063785008236851,\n",
       "  0.0011069972105861242,\n",
       "  0.0011037845385660425,\n",
       "  0.0011088203857952908,\n",
       "  0.0011099064567812423,\n",
       "  0.001113107506306945,\n",
       "  0.0011089575844429522,\n",
       "  0.0011085213724762427,\n",
       "  0.0011083517686142582,\n",
       "  0.0011078803300791724,\n",
       "  0.0011129722604383402,\n",
       "  0.0011078626976695428,\n",
       "  0.001104006280779825,\n",
       "  0.0011056975067549049,\n",
       "  0.0011052657278929677,\n",
       "  0.0011022403933220759,\n",
       "  0.0011010399665043128,\n",
       "  0.0011021635882498613,\n",
       "  0.001103612966378442,\n",
       "  0.0011034788157742459,\n",
       "  0.0011034831250407043,\n",
       "  0.0011000798803321467,\n",
       "  0.001099187495652521,\n",
       "  0.001103672068000937,\n",
       "  0.0011045941950389067,\n",
       "  0.0011028622502615986,\n",
       "  0.0011075732566201695,\n",
       "  0.001113628959966354,\n",
       "  0.001109620089395593,\n",
       "  0.001113045919141025,\n",
       "  0.0011172973065973256,\n",
       "  0.0011128822290535207,\n",
       "  0.0011195011958327148,\n",
       "  0.001121347459371876,\n",
       "  0.0011202131507440914,\n",
       "  0.0011211350791860883,\n",
       "  0.0011177522926219816,\n",
       "  0.0011212914129055376,\n",
       "  0.0011212540531440349,\n",
       "  0.0011227048766782445,\n",
       "  0.0011211351161472018,\n",
       "  0.001119299863546645,\n",
       "  0.0011174712205460238,\n",
       "  0.0011161934781607594,\n",
       "  0.0011140863930231733,\n",
       "  0.0011145367921449954,\n",
       "  0.0011135815367251676,\n",
       "  0.0011117042468014975,\n",
       "  0.0011127213763684372,\n",
       "  0.001113292633118427,\n",
       "  0.0011114529363008886,\n",
       "  0.0011094045673478495,\n",
       "  0.0011104733786495772,\n",
       "  0.0011123482178629282,\n",
       "  0.0011117612103886507,\n",
       "  0.0011176832519370656,\n",
       "  0.0011195292118910212,\n",
       "  0.0011173728122186955,\n",
       "  0.001113767044032548,\n",
       "  0.0011101462846093266,\n",
       "  0.0011086134891876832,\n",
       "  0.0011117435200081781,\n",
       "  0.001111868147688234,\n",
       "  0.0011065399958670977,\n",
       "  0.0011071777775112223,\n",
       "  0.0011106404885331636,\n",
       "  0.0011107365618255453,\n",
       "  0.0011167054945025834,\n",
       "  0.0011189376356498693,\n",
       "  0.0011220704608265936,\n",
       "  0.0011236963590474118,\n",
       "  0.00112397245725163,\n",
       "  0.0011281515592194784,\n",
       "  0.0011326443778161346,\n",
       "  0.0011317000285149242,\n",
       "  0.001130538060568175,\n",
       "  0.0011345585552200103,\n",
       "  0.0011356355462144878,\n",
       "  0.0011372696847256785,\n",
       "  0.0011363144689258476,\n",
       "  0.0011339324304942893,\n",
       "  0.0011322454691402128]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "lightgbm.cv(best_params, train_data, 1000, nfold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = lightgbm.train(best_params, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04217453, 0.06574535, 0.01996266, ..., 0.18041187, 0.11049542,\n",
       "       0.08077953])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bst.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19044 0.04049\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y_train), np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = lightgbm.LGBMClassifier(\n",
    "    num_leaves=best_params['num_leaves'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_samples=best_params['min_child_samples'],\n",
    "    reg_alpha=best_params['lambda_l1'],\n",
    "    reg_lambda=best_params['lambda_l2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(max_depth=42, min_child_samples=3775, num_leaves=1190,\n",
       "               reg_alpha=0.021477231281508247, reg_lambda=0.0394564387769446)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(max_depth=42, min_child_samples=3775, num_leaves=1190,\n",
       "               reg_alpha=0.021477231281508247, reg_lambda=0.0394564387769446)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(max_depth=42, min_child_samples=3775, num_leaves=1190,\n",
       "               reg_alpha=0.021477231281508247, reg_lambda=0.0394564387769446)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = bst.predict(X_train)\n",
    "y_pred = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.028)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
